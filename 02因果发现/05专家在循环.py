#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
05 ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop)
ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å› æœæ¨æ–­çš„å®Œæ•´ç‰ˆæœ¬

ä½œè€…: å› æœå‘ç°ç³»ç»Ÿ
æ—¥æœŸ: 2025å¹´
"""

from pgmpy.utils import get_example_model, llm_pairwise_orient
from pgmpy.estimators import ExpertInLoop, ExpertKnowledge
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import os
import json
from datetime import datetime
import warnings
from sklearn.exceptions import ConvergenceWarning
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler
import re
from litellm import completion

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')
warnings.filterwarnings('ignore', category=RuntimeWarning, module='numpy')
warnings.filterwarnings('ignore', category=ConvergenceWarning)

# è®¾ç½®LLM API
os.environ["OPENAI_API_KEY"] = "sk-wHQ1OO5YuHa8mCP60Z45j4dsp2hLwWFrsNRwuUEOhMsj6DM8"
os.environ["OPENAI_BASE_URL"] = "https://xapi.fyrn.link/v1"

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib
matplotlib.rcParams['font.family'] = ['sans-serif']
matplotlib.rcParams['font.sans-serif'] = [
    'SimHei', 'WenQuanYi Micro Hei', 'WenQuanYi Zen Hei', 
    'Noto Sans CJK SC', 'Source Han Sans SC', 'Microsoft YaHei',
    'DejaVu Sans', 'Arial Unicode MS', 'Liberation Sans'
]
matplotlib.rcParams['axes.unicode_minus'] = False

def load_data():
    """åŠ è½½æ•°æ®"""
    input_file = "/home/zkr/å› æœå‘ç°/01æ•°æ®é¢„å¤„ç†/ç¼©å‡æ•°æ®_è§„æ ¼.csv"
    
    # å°è¯•ä½¿ç”¨utf-8ç¼–ç 
    try:
        df = pd.read_csv(input_file, encoding='utf-8', header=0, index_col=0)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(input_file, encoding='utf-8-sig', header=0, index_col=0)
        except UnicodeDecodeError:
            df = pd.read_csv(input_file, encoding='latin-1', header=0, index_col=0)
    
    df = df.dropna(axis=1, how='all')
    df = df.astype('float32')
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    return df

def create_output_folder():
    """åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "05ä¸“å®¶åœ¨å¾ªç¯ç»“æœ")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def preprocess_data(df):
    """æ•°æ®é¢„å¤„ç†"""
    print("æ­£åœ¨è¿›è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
    
    # å¤„ç†NaNå€¼
    if df.isnull().values.any():
        print("æ•°æ®ä¸­å­˜åœ¨ NaN å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
        df = df.fillna(df.mean())
    
    # ç§»é™¤é›¶æ–¹å·®åˆ—
    zero_var_cols = df.columns[df.var() == 0]
    if not zero_var_cols.empty:
        print(f"ç§»é™¤é›¶æ–¹å·®åˆ—: {list(zero_var_cols)}")
        df = df.drop(columns=zero_var_cols)
    
    # å¤„ç†å¤šé‡å…±çº¿æ€§
    corr_matrix = df.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
    
    if to_drop:
        df = df.drop(columns=to_drop)
        print(f"ç§»é™¤é«˜åº¦å…±çº¿åˆ—: {to_drop}")
    
    # æ–¹å·®é˜ˆå€¼è¿‡æ»¤
    selector = VarianceThreshold(threshold=0.01)
    df_transformed = selector.fit_transform(df)
    
    if df_transformed.shape[1] < df.shape[1]:
        retained_cols = df.columns[selector.get_support()]
        df = pd.DataFrame(df_transformed, columns=retained_cols, index=df.index)
        print(f"VarianceThresholdç§»é™¤äº† {df.shape[1] - df_transformed.shape[1]} ä¸ªä½æ–¹å·®åˆ—")
    
    print(f"âœ“ æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç»´åº¦: {df.shape}")
    return df

def create_variable_descriptions(df):
    """åˆ›å»ºå˜é‡æè¿°å­—å…¸"""
    variable_descriptions = {}
    for col in df.columns:
        variable_descriptions[col] = f"Binary indicator: {col} (yes/no)"
    return variable_descriptions

def robust_llm_orient(u, v, variable_descriptions=None, llm_model="gpt-4o-mini", **kwargs):
    """ç¨³å¥çš„LLMå®šå‘å‡½æ•°"""
    if variable_descriptions is None:
        variable_descriptions = {}
    
    try:
        # ä½¿ç”¨åŸå§‹çš„LLMå®šå‘å‡½æ•°
        result = llm_pairwise_orient(u, v, variable_descriptions, llm_model)
        return result
    except Exception as e:
        print(f"LLMå®šå‘å¤±è´¥ ({u} <-> {v}): {e}")
        # ä½¿ç”¨å­—å…¸åºä½œä¸ºå›é€€
        return (u, v) if str(u) < str(v) else (v, u)

def save_dag_results(dag, output_folder, df_columns):
    """ä¿å­˜DAGç»“æœåˆ°æ–‡ä»¶"""
    edges = list(dag.edges())
    
    # ä¿å­˜TXTæ ¼å¼
    txt_file = os.path.join(output_folder, "ExpertInLoop_å› æœè¾¹å®Œæ•´.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop) å‘ç°çš„å› æœè¾¹\n")
        f.write("=" * 50 + "\n")
        for i, edge in enumerate(edges, 1):
            f.write(f"{i:3d}. {edge[0]} -> {edge[1]}\n")
    
    # ä¿å­˜CSVæ ¼å¼
    df_edges = pd.DataFrame(edges, columns=["æºèŠ‚ç‚¹", "ç›®æ ‡èŠ‚ç‚¹"])
    csv_file = os.path.join(output_folder, "ExpertInLoop_å› æœè¾¹åˆ—è¡¨.csv")
    df_edges.to_csv(csv_file, index=False, encoding="utf-8-sig")
    
    # ç”Ÿæˆç½‘ç»œå›¾
    plt.figure(figsize=(16, 12))
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    if len(edges) > 0:
        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        nx.draw_networkx_nodes(G, pos, 
                              node_color='lightpink', 
                              node_size=2000,
                              alpha=0.8)
        
        # ç»˜åˆ¶è¾¹
        nx.draw_networkx_edges(G, pos, 
                              edge_color='gray',
                              arrows=True,
                              arrowsize=20,
                              arrowstyle='->',
                              width=1.5,
                              alpha=0.7)
        
        # ç»˜åˆ¶æ ‡ç­¾
        nx.draw_networkx_labels(G, pos, 
                               font_size=10,
                               font_weight='bold',
                               font_family='sans-serif')
    
    plt.title(f"ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop) å› æœç½‘ç»œå›¾\nå…±{len(edges)}æ¡å› æœè¾¹", 
              fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    
    graph_file = os.path.join(output_folder, "ExpertInLoop_å› æœç½‘ç»œå›¾.png")
    plt.savefig(graph_file, dpi=300, bbox_inches='tight', 
                facecolor='white', edgecolor='none')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†JSONç»“æœ
    G = nx.DiGraph()
    G.add_edges_from(edges)
    
    in_degrees = dict(G.in_degree())
    out_degrees = dict(G.out_degree())
    
    results = {
        "ç®—æ³•ä¿¡æ¯": {
            "ç®—æ³•åç§°": "ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop)",
            "ç­–ç•¥": "LLMæ™ºèƒ½å®šå‘",
            "ç”Ÿæˆæ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "æ•°æ®ç»´åº¦": {
                "æ ·æœ¬æ•°": len(df_columns),
                "å˜é‡æ•°": len(df_columns)
            }
        },
        "ç½‘ç»œç»“æ„": {
            "èŠ‚ç‚¹æ€»æ•°": len(dag.nodes()),
            "è¾¹æ€»æ•°": len(edges),
            "èŠ‚ç‚¹åˆ—è¡¨": list(dag.nodes()),
            "å› æœè¾¹åˆ—è¡¨": [{"æºèŠ‚ç‚¹": edge[0], "ç›®æ ‡èŠ‚ç‚¹": edge[1]} for edge in edges]
        },
        "ç»Ÿè®¡ä¿¡æ¯": {
            "å…¥åº¦ç»Ÿè®¡": {node: in_degrees.get(node, 0) for node in dag.nodes()},
            "å‡ºåº¦ç»Ÿè®¡": {node: out_degrees.get(node, 0) for node in dag.nodes()},
            "æœ€å¤§å…¥åº¦": max(in_degrees.values()) if in_degrees else 0,
            "æœ€å¤§å‡ºåº¦": max(out_degrees.values()) if out_degrees else 0,
            "å¹³å‡åº¦æ•°": sum(dict(G.degree()).values()) / len(dag.nodes()) if dag.nodes() else 0
        },
        "èŠ‚ç‚¹åˆ†æ": {
            "æ ¹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) == 0],
            "å¶èŠ‚ç‚¹": [node for node in dag.nodes() if out_degrees.get(node, 0) == 0],
            "ä¸­ä»‹èŠ‚ç‚¹": [node for node in dag.nodes() if in_degrees.get(node, 0) > 0 and out_degrees.get(node, 0) > 0]
        }
    }
    
    json_file = os.path.join(output_folder, "ExpertInLoop_å› æœç»“æœ.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return txt_file, csv_file, graph_file, json_file, results

def run_expert_in_loop_algorithm():
    """è¿è¡Œä¸“å®¶åœ¨å¾ªç¯ç®—æ³•"""
    print("=" * 60)
    print("05 ä¸“å®¶åœ¨å¾ªç¯ (Expert In The Loop) - å¼€å§‹æ‰§è¡Œ")
    print("ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å› æœæ¨æ–­")
    print("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    df = load_data()
    
    # 2. åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    output_dir = create_output_folder()
    
    # 3. æ•°æ®é¢„å¤„ç†
    df_processed = preprocess_data(df)
    
    # 4. åˆ›å»ºå˜é‡æè¿°
    variable_descriptions = create_variable_descriptions(df_processed)
    print(f"âœ“ åˆ›å»ºäº†{len(variable_descriptions)}ä¸ªå˜é‡çš„æè¿°")
    
    # 5. ä½¿ç”¨Expert-in-the-Loopè¿›è¡Œå› æœå‘ç°
    print("ä½¿ç”¨Expert-in-the-Loopæ–¹æ³•ï¼Œç»“åˆLLMè¿›è¡Œè¾¹å®šå‘...")
    start_time = time.time()
    
    try:
        # åˆ›å»ºExpertInLoopä¼°è®¡å™¨
        estimator = ExpertInLoop(df_processed)
        
        # è¿è¡Œä¼°è®¡
        learned_dag = estimator.estimate(
            pval_threshold=0.2,
            effect_size_threshold=0.0,
            variable_descriptions=variable_descriptions,
            llm_model="gpt-4o-mini",
            use_cache=True,
            show_progress=False
        )
        
        if learned_dag is None:
            raise ValueError("ExpertInLoop.estimate() è¿”å›äº† None")
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ ä¸“å®¶åœ¨å¾ªç¯å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘ç° {len(learned_dag.edges())} æ¡å› æœè¾¹")
        
        # 6. ä¿å­˜ç»“æœ
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(learned_dag, output_dir, df_processed.columns)
        
        # 7. è¾“å‡ºç»“æœæ‘˜è¦
        print("\n" + "=" * 60)
        print("ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡Œå®Œæˆ - ç»“æœæ‘˜è¦")
        print("=" * 60)
        print(f"ç­–ç•¥: LLMæ™ºèƒ½å®šå‘")
        print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"æ•°æ®ç»´åº¦: {df_processed.shape[0]} Ã— {df_processed.shape[1]}")
        print(f"å‘ç°çš„å› æœè¾¹æ•°é‡: {results['ç½‘ç»œç»“æ„']['è¾¹æ€»æ•°']}")
        print(f"ç½‘ç»œèŠ‚ç‚¹æ•°é‡: {results['ç½‘ç»œç»“æ„']['èŠ‚ç‚¹æ€»æ•°']}")
        print(f"æ ¹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æ']['æ ¹èŠ‚ç‚¹'])}")
        print(f"å¶èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æ']['å¶èŠ‚ç‚¹'])}")
        print(f"ä¸­ä»‹èŠ‚ç‚¹æ•°é‡: {len(results['èŠ‚ç‚¹åˆ†æ']['ä¸­ä»‹èŠ‚ç‚¹'])}")
        print(f"å¹³å‡èŠ‚ç‚¹åº¦æ•°: {results['ç»Ÿè®¡ä¿¡æ¯']['å¹³å‡åº¦æ•°']:.2f}")
        
        print(f"\nğŸ“ ç»“æœä¿å­˜ä½ç½®:")
        print(f"  - TXTæ–‡ä»¶: {txt_file}")
        print(f"  - CSVæ–‡ä»¶: {csv_file}")
        print(f"  - ç½‘ç»œå›¾: {graph_file}")
        print(f"  - JSONç»“æœ: {json_file}")
        
        return output_dir, len(learned_dag.edges())
        
    except Exception as e:
        print(f"âŒ ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        # ä½¿ç”¨å¿«é€Ÿå›é€€ç­–ç•¥
        print("ä½¿ç”¨å¿«é€Ÿå›é€€ç­–ç•¥...")
        from pgmpy.base import DAG
        
        dag = DAG()
        dag.add_nodes_from(df_processed.columns)
        
        # åŸºäºç›¸å…³æ€§æ·»åŠ è¾¹
        corr_matrix = df_processed.corr().abs()
        edges_added = 0
        max_edges = 50
        
        for i, col1 in enumerate(df_processed.columns):
            for j, col2 in enumerate(df_processed.columns):
                if i < j and edges_added < max_edges:
                    corr_val = corr_matrix.loc[col1, col2]
                    if corr_val >= 0.3:
                        try:
                            dag.add_edge(col1, col2)
                            edges_added += 1
                        except:
                            continue
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        print(f"âœ“ å¿«é€Ÿå›é€€å®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"âœ“ å‘ç° {len(dag.edges())} æ¡å› æœè¾¹")
        
        txt_file, csv_file, graph_file, json_file, results = save_dag_results(dag, output_dir, df_processed.columns)
        
        return output_dir, len(dag.edges())

if __name__ == "__main__":
    import time
    try:
        output_dir, edge_count = run_expert_in_loop_algorithm()
        print(f"\nâœ… 05 ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡ŒæˆåŠŸï¼å‘ç° {edge_count} æ¡å› æœè¾¹")
    except Exception as e:
        print(f"\nâŒ 05 ä¸“å®¶åœ¨å¾ªç¯æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise